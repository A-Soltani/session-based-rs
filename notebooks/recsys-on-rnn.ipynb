{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21203f93",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd76a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6b6cf",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e04fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of model. Used for saving conventions\n",
    "name = 'recsys' # 'imusic'\n",
    "\n",
    "# set sise of data (number of samples). If None (suggested), full datasets are applied.\n",
    "limit = None\n",
    "\n",
    "# how often would you like to check results?\n",
    "show_every_n_batches = 3000\n",
    "\n",
    "# decide on wether to show full validation statistics. Computational time is high when this is True\n",
    "full_validation_stats = False\n",
    "\n",
    "# decide whether to log testing\n",
    "log_testing = True\n",
    "\n",
    "# top k products to determine accuracy\n",
    "top_k = 20\n",
    "\n",
    "notes = 'Final GRU Model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec0bc4",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c63a7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 512 - Number of sequences running through the network in one pass.\n",
    "batch_size = 512\n",
    "\n",
    "# 50 - Embedding dimensions\n",
    "embed_dim = 300\n",
    "\n",
    "# The dropout drop probability when training on input. If you're network is overfitting, try decreasing this.\n",
    "x_drop_probability = 0.00\n",
    "\n",
    "# The dropout keep probability when training on RNN neurons. If you're network is overfitting, try decreasing this.\n",
    "rnn_keep_probability = 1.00\n",
    "\n",
    "# 100 - The number of units in the hidden layers.\n",
    "rnn_size = 200\n",
    "\n",
    "# 1\n",
    "num_layers = 1\n",
    "\n",
    "# Learning rate for training\n",
    "# typically 0.0001 up to 1: http://datascience.stackexchange.com/questions/410/choosing-a-learning-rate\n",
    "# best learning_rate = 0.0025\n",
    "learning_rate = 0.0025\n",
    "\n",
    "# 10 epochs\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d9c9ca",
   "metadata": {},
   "source": [
    "# Create model folder for hyperparameters, statistics and the model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9aafbd",
   "metadata": {},
   "source": [
    "## Update and get model_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c5da9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_counter_path = '../models/model_counter.txt'\n",
    "# os.path.isfile() method in Python is used to check whether the specified path is an existing regular file or not.\n",
    "if os.path.isfile(model_counter_path):\n",
    "    # The open() function returns a file object, which has a read() method for reading the content of the file\n",
    "    # Read Only (‘r’)\n",
    "    model_counter_file = open(model_counter_path, 'r') \n",
    "    model_count = int(model_counter_file.read())\n",
    "    model_counter_file.close()\n",
    "    # Write Only (‘w’)\n",
    "    model_counter_file = open(model_counter_path, 'w')\n",
    "    model_counter_file.write(str(model_count + 1))\n",
    "    model_counter_file.close()\n",
    "else:\n",
    "    # Write and Read (‘w+’)\n",
    "    model_counter_file = open(model_counter_path, 'w+')\n",
    "    model_count = 1000 # initial model count/number\n",
    "    model_counter_file.write(str(model_count + 1))\n",
    "    model_counter_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3d6e8",
   "metadata": {},
   "source": [
    "## Make model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d923f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_dir = '../models/model_count/' + str(model_count) + '-' + name + '-' + time.strftime(\"%y%m%d\") + '/'\n",
    "if not os.path.exists(model_path_dir):\n",
    "    os.makedirs(model_path_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34bb40",
   "metadata": {},
   "source": [
    "## Update stats_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c6bb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file_path = model_path_dir + name + '-' + time.strftime(\"%y%m%d%H%M\") + '-statsfile' + '.txt'\n",
    "stats_file = open(stats_file_path, 'w+')\n",
    "stats_file.write('model number: {}\\n'.format(model_count))\n",
    "stats_file.write('name: {}\\n\\n'.format(name))\n",
    "stats_file.write('limit: {}\\n'.format(limit))\n",
    "stats_file.write('batch_size: {}\\n'.format(batch_size))\n",
    "stats_file.write('embed_dim: {}\\n'.format(embed_dim))\n",
    "stats_file.write('x_drop_probability: {}\\n'.format(x_drop_probability))\n",
    "stats_file.write('rnn_keep_probability: {}\\n'.format(rnn_keep_probability))\n",
    "stats_file.write('rnn_size: {}\\n'.format(rnn_size))\n",
    "stats_file.write('num_layers: {}\\n'.format(num_layers))\n",
    "stats_file.write('learning_rate: {}\\n'.format(learning_rate))\n",
    "stats_file.write('num_epochs: {}\\n'.format(num_epochs))\n",
    "stats_file.write('show_every_n_batches: {}\\n'.format(show_every_n_batches))\n",
    "stats_file.write('top_k: {}\\n'.format(top_k))\n",
    "stats_file.write('full_validation_stats: {}\\n'.format(full_validation_stats))\n",
    "stats_file.write('notes: {}\\n'.format(notes))\n",
    "stats_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554dd7e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea67228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_our_data(path, limit):\n",
    "    return pd.read_csv(path, nrows = limit, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1d5a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if limit == None:\n",
    "    validation_limit = None\n",
    "    testing_limit = None\n",
    "else:\n",
    "    validation_limit = int(0.2 * limit)\n",
    "    testing_limit = int(0.2 * limit)\n",
    "\n",
    "prepared_data_path = \"../data/rsc15/prepared/\"\n",
    "\n",
    "tr_data = load_our_data(path=f\"{prepared_data_path}yoochoose-clicks-100k_train_full.txt\", limit=limit)\n",
    "va_data = load_our_data(path=f\"{prepared_data_path}yoochoose-clicks-100k_train_valid.txt\", limit=validation_limit)\n",
    "te_data = load_our_data(path=f\"{prepared_data_path}yoochoose-clicks-100k_test.txt\", limit=testing_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a7651",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1f030",
   "metadata": {},
   "source": [
    "## Get unique items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dba0207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques in training   2933\n",
      "uniques in validation 2029\n",
      "uniques in testing    1771\n",
      "\n",
      "depth (unique items)  2933\n"
     ]
    }
   ],
   "source": [
    "# get number of unique products\n",
    "print('uniques in training  ', np.unique(tr_data['ItemId']).shape[0])\n",
    "print('uniques in validation', np.unique(va_data['ItemId']).shape[0])\n",
    "print('uniques in testing   ', np.unique(te_data['ItemId']).shape[0])\n",
    "\n",
    "# unique item_ids\n",
    "uniques = np.unique(np.append(np.append(tr_data['ItemId'], va_data['ItemId']), te_data['ItemId']))\n",
    "depth = uniques.shape[0]\n",
    "print('\\ndepth (unique items) ', depth)\n",
    "if depth != np.unique(tr_data['ItemId']).shape[0]:\n",
    "    print('\\nWARNING! Number of uniques in training should equal the depth (uniques in full set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f9a3c",
   "metadata": {},
   "source": [
    "## Creating a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fede49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(item_ids):    \n",
    "    \n",
    "    items_to_int = pd.Series(data=np.arange(len(item_ids)),index=item_ids)\n",
    "    int_to_items = pd.DataFrame({\"ItemId\":item_ids,'item_idx':items_to_int[item_ids].values})\n",
    "    \n",
    "    return items_to_int, int_to_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f537c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_to_int, int_to_items = create_lookup_tables(list(uniques))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d697542",
   "metadata": {},
   "source": [
    "## Transforming and splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea479c1",
   "metadata": {},
   "source": [
    "In each session, the number of events\n",
    "is also referred to as the number of timesteps in a session. Sessions with a single\n",
    "timestep (one event) are dropped as it is not possible to train a model on inputs\n",
    "with no targets. \n",
    "\n",
    "The remaining sessions will be split into input (**X**) and target values (**y**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e396b91",
   "metadata": {},
   "source": [
    "## Session Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fad030",
   "metadata": {},
   "source": [
    "The span of session lengths can be a problem for recurrent neural networks as they require fixed sized inputs.\n",
    "The main proportion of the RecSys and AVM sessions spans **19** or fewer events.\n",
    "\n",
    "Sessions with more than 19 timesteps are split into multiple sessions and act as separate independent sessions. Loss of information by splitting long sessions is substantially low and the advantages of a much higher computation speed is valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f9eca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 - Number of timesteps the rnn should take in\n",
    "timesteps = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61061146",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220d7d4",
   "metadata": {},
   "source": [
    "A method called zero padding makes sessions the same length by adding zeros to missing timesteps in sessions shorter than n (Hearty, 2016). Padding is later reversed by masking, which ensures the added zeros of padding have no effect on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfc86a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SessionId     ItemId          Time\n",
      "0              3  214716935  1.396437e+09\n",
      "1              3  214832672  1.396438e+09\n",
      "2              6  214701242  1.396796e+09\n",
      "3              6  214826623  1.396797e+09\n",
      "4              7  214826835  1.396414e+09\n",
      "...          ...        ...           ...\n",
      "68914      32764  214717567  1.396431e+09\n",
      "68915      32764  214717567  1.396431e+09\n",
      "68911      32766  214585554  1.396711e+09\n",
      "68912      32766  214585554  1.396711e+09\n",
      "68913      32766  214819762  1.396711e+09\n",
      "\n",
      "[70278 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "session_key='SessionId'\n",
    "item_key='ItemId'\n",
    "time_key='Time'\n",
    "\n",
    "tr_data.sort_values([session_key, time_key], inplace=True)\n",
    "print(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19e5835a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SessionId\n",
      "3        2\n",
      "6        2\n",
      "7        2\n",
      "8        2\n",
      "9        3\n",
      "        ..\n",
      "32759    4\n",
      "32762    2\n",
      "32763    6\n",
      "32764    2\n",
      "32766    3\n",
      "Length: 17794, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test= tr_data.groupby(session_key).size().cumsum() \n",
    "test1= tr_data.groupby(session_key)\n",
    "\n",
    "print(test1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e33ec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     4     6 ... 70273 70275 70278]\n",
      "[    0     2     4 ... 70273 70275 70278]\n"
     ]
    }
   ],
   "source": [
    "offsets = np.zeros(tr_data[session_key].nunique() + 1, dtype=np.int32)\n",
    "# group & sort the df by session_key and get the offset values\n",
    "offsets[1:] = tr_data.groupby(session_key).size().cumsum()\n",
    "\n",
    "print(offsets[1:])\n",
    "print(offsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ba2d18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SessionId\n",
      "3        2\n",
      "6        2\n",
      "7        2\n",
      "8        2\n",
      "9        3\n",
      "        ..\n",
      "32759    4\n",
      "32762    2\n",
      "32763    6\n",
      "32764    2\n",
      "32766    3\n",
      "Length: 17794, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "t1 = tr_data.groupby('SessionId').size()\n",
    "print(t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c024250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SessionId     ItemId          Time\n",
      "0              3  214716935  1.396437e+09\n",
      "1              3  214832672  1.396438e+09\n",
      "2              6  214701242  1.396796e+09\n",
      "3              6  214826623  1.396797e+09\n",
      "4              7  214826835  1.396414e+09\n",
      "...          ...        ...           ...\n",
      "68914      32764  214717567  1.396431e+09\n",
      "68915      32764  214717567  1.396431e+09\n",
      "68911      32766  214585554  1.396711e+09\n",
      "68912      32766  214585554  1.396711e+09\n",
      "68913      32766  214819762  1.396711e+09\n",
      "\n",
      "[70278 rows x 3 columns]\n",
      "       SessionId     ItemId          Time\n",
      "8              9  214576500  1.396776e+09\n",
      "9              9  214576500  1.396777e+09\n",
      "10             9  214576500  1.396777e+09\n",
      "11            11  214821275  1.396515e+09\n",
      "12            11  214821275  1.396515e+09\n",
      "...          ...        ...           ...\n",
      "68920      32763  214552151  1.396611e+09\n",
      "68921      32763  214552151  1.396611e+09\n",
      "68911      32766  214585554  1.396711e+09\n",
      "68912      32766  214585554  1.396711e+09\n",
      "68913      32766  214819762  1.396711e+09\n",
      "\n",
      "[54926 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data = tr_data\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths>=3].index)]\n",
    "print(tr_data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rare_clicked_items(data):\n",
    "    \n",
    "    #delete records of items which appeared less than 5 times\n",
    "    itemLen = data.groupby('ItemID').size() #groupby itemID and get size of each item\n",
    "    data = data[np.in1d(train.ItemID, itemLen[itemLen > 4].index)]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "126045b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_single_timestep_sessions(data):\n",
    "    \n",
    "    ''' Sessions with a single timestep (one event) are dropped \n",
    "    as it is not possible to train a model on inputs with no targets '''\n",
    "    session_lengths = data.groupby('SessionId').size()\n",
    "    data = data[np.in1d(data.SessionId, session_lengths[session_lengths>1].index)]\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f79bbe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_split_our_data(data, timesteps):\n",
    "    \n",
    "    drop_single_timestep_sessions(data)\n",
    "    \n",
    "    # The remaining sessions will be split into input and target values.\n",
    "    \n",
    "    # After puting events with same session as a group, group length should be checked\n",
    "    # if length < timesteps (19) padding should be used, adding some ziro ???\n",
    "    # if length > timesteps, these sessions should be split into multiple sessions having 19 length\n",
    "    # Loss of information by splitting long sessions is substantially low \n",
    "    # and the advantages of a much higher computation speed is valuable.a\n",
    "    \n",
    "    # Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "    # clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "    data.sort_values([session_key, time_key], inplace=True)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f533de90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AL9107~1.SOL\\AppData\\Local\\Temp/ipykernel_22436/3472079369.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Transforming and splitting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_and_split_our_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_va\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_va\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_and_split_our_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mva_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_and_split_our_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\AL9107~1.SOL\\AppData\\Local\\Temp/ipykernel_22436/3316626429.py\u001b[0m in \u001b[0;36mtransform_and_split_our_data\u001b[1;34m(data, timesteps)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtransform_and_split_our_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdrop_single_timestep_sessions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# The remaining sessions will be split into input and target values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\AL9107~1.SOL\\AppData\\Local\\Temp/ipykernel_22436/336900692.py\u001b[0m in \u001b[0;36mdrop_single_timestep_sessions\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Sessions with a single timestep (one event) are dropped as it is not possible to train a model on inputs with no targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# Transforming and splitting the data\n",
    "X_tr, y_tr = transform_and_split_our_data(tr_data, timesteps)\n",
    "X_va, y_va = transform_and_split_our_data(va_data, timesteps)\n",
    "X_te, y_te = transform_and_split_our_data(te_data, timesteps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
