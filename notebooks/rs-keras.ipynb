{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6dbe382",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6342ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471ab3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346507e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b0739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_our_data(path, limit):\n",
    "    return pd.read_csv(path, nrows = limit, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b5e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sise of data (number of samples). If None (suggested), full datasets are applied.\n",
    "limit = None\n",
    "validation_limit = None\n",
    "testing_limit = None\n",
    "\n",
    "if limit == None:\n",
    "    validation_limit = None\n",
    "    testing_limit = None\n",
    "else:\n",
    "    validation_limit = int(0.2 * limit)\n",
    "    testing_limit = int(0.2 * limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd58bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_path = \"../data/rsc15/prepared/\"\n",
    "\n",
    "train_data = load_our_data(path=f\"{prepared_data_path}yoochoose-clicks-100k_train_full.txt\", limit=limit)\n",
    "dev_data = load_our_data(path=f\"{prepared_data_path}yoochoose-clicks-100k_train_valid.txt\", limit=validation_limit)\n",
    "test_data = load_our_data(path=f\"{prepared_data_path}yoochoose-clicks-100k_test.txt\", limit=testing_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce336369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>214716935</td>\n",
       "      <td>1.396437e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>214832672</td>\n",
       "      <td>1.396438e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>214701242</td>\n",
       "      <td>1.396796e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>214826623</td>\n",
       "      <td>1.396797e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>214826835</td>\n",
       "      <td>1.396414e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70273</th>\n",
       "      <td>31813</td>\n",
       "      <td>214691293</td>\n",
       "      <td>1.396769e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70274</th>\n",
       "      <td>31812</td>\n",
       "      <td>214662819</td>\n",
       "      <td>1.396365e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70275</th>\n",
       "      <td>31812</td>\n",
       "      <td>214836765</td>\n",
       "      <td>1.396365e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70276</th>\n",
       "      <td>31812</td>\n",
       "      <td>214836073</td>\n",
       "      <td>1.396365e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70277</th>\n",
       "      <td>31812</td>\n",
       "      <td>214662819</td>\n",
       "      <td>1.396365e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70278 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SessionId     ItemId          Time\n",
       "0              3  214716935  1.396437e+09\n",
       "1              3  214832672  1.396438e+09\n",
       "2              6  214701242  1.396796e+09\n",
       "3              6  214826623  1.396797e+09\n",
       "4              7  214826835  1.396414e+09\n",
       "...          ...        ...           ...\n",
       "70273      31813  214691293  1.396769e+09\n",
       "70274      31812  214662819  1.396365e+09\n",
       "70275      31812  214836765  1.396365e+09\n",
       "70276      31812  214836073  1.396365e+09\n",
       "70277      31812  214662819  1.396365e+09\n",
       "\n",
       "[70278 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331b39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_items = len(train_data['ItemId'].unique()) + 1\n",
    "train_samples_qty = len(train_data['SessionId'].unique()) + 1\n",
    "test_samples_qty = len(test_data['SessionId'].unique()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a40d7",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c12b127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 512 - Number of sequences running through the network in one pass.\n",
    "batch_size = 512\n",
    "\n",
    "# 50 - Embedding dimensions\n",
    "embed_dim = 300\n",
    "\n",
    "# The dropout drop probability when training on input. If you're network is overfitting, try decreasing this.\n",
    "x_drop_probability = 0.00\n",
    "\n",
    "# The dropout keep probability when training on RNN neurons. If you're network is overfitting, try decreasing this.\n",
    "rnn_keep_probability = 1.00\n",
    "\n",
    "# 100 - The number of units in the hidden layers.\n",
    "rnn_size = 200\n",
    "\n",
    "# 1\n",
    "num_layers = 1\n",
    "\n",
    "# Learning rate for training\n",
    "# typically 0.0001 up to 1: http://datascience.stackexchange.com/questions/410/choosing-a-learning-rate\n",
    "# best learning_rate = 0.0025\n",
    "learning_rate = 0.0025\n",
    "\n",
    "# 20 epochs\n",
    "epochs = 20\n",
    "\n",
    "# save model\n",
    "save_weights = False\n",
    "\n",
    "# eval_all_epochs\n",
    "eval_all_epochs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2c0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"    \n",
    "    def __init__(self, data, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_samples=-1, itemmap=None, time_sort=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path of the csv file\n",
    "            sep: separator for the csv\n",
    "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            itemmap: mapping between item IDs and item indices\n",
    "            time_sort: whether to sort the sessions by time or not\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "\n",
    "        # Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        # clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "        \n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each sessions, sorted by session IDs\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "\n",
    "        return session_idx_arr\n",
    "    \n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\" \n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "        \n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "        \n",
    "    @property    \n",
    "    def items(self):\n",
    "        return self.itemmap.ItemId.unique()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af67d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataLoader:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"    \n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "            dataset (SessionDataset): the session dataset to generate the batches from\n",
    "            batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_sessions_counter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.dataset.df\n",
    "        session_key='SessionId'\n",
    "        item_key='ItemId'\n",
    "        time_key='TimeStamp'\n",
    "        self.n_items = df[item_key].nunique()+1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = [] # indicator for the sessions to be terminated\n",
    "        finished = False        \n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices (for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                inp = idx_input\n",
    "                target = idx_target\n",
    "                yield inp, target, mask\n",
    "                \n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_sessions_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c25bdb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():   \n",
    "    emb_size = 50\n",
    "    hidden_units = 100\n",
    "    size = emb_size\n",
    "\n",
    "    inputs = Input(batch_shape=(batch_size, 1, train_n_items))\n",
    "    gru, gru_states = GRU(hidden_units, stateful=True, return_state=True, name=\"GRU\")(inputs)\n",
    "    drop2 = Dropout(0.25)(gru)\n",
    "    predictions = Dense(train_n_items, activation='softmax')(drop2)\n",
    "    model = Model(inputs=inputs, outputs=[predictions])\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
    "    model.summary()\n",
    "\n",
    "    filepath='./model_checkpoint.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "    callbacks_list = []\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72b41d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, train_generator_map, recall_k=20, mrr_k=20):\n",
    "\n",
    "    test_dataset = SessionDataset(test_data, itemmap=train_generator_map)\n",
    "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    n = 0\n",
    "    rec_sum = 0\n",
    "    mrr_sum = 0\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    for feat, label, mask in test_generator:\n",
    "\n",
    "        gru_layer = model.get_layer(name=\"GRU\")\n",
    "        hidden_states = gru_layer.states[0].numpy()\n",
    "        for elt in mask:\n",
    "            hidden_states[elt, :] = 0\n",
    "        gru_layer.reset_states(states=hidden_states)\n",
    "\n",
    "        target_oh = to_categorical(label, num_classes=train_n_items)\n",
    "        input_oh  = to_categorical(feat,  num_classes=train_n_items)\n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "\n",
    "        pred = model.predict(input_oh, batch_size=batch_size)\n",
    "\n",
    "        for row_idx in range(feat.shape[0]):\n",
    "            pred_row = pred[row_idx]\n",
    "            label_row = target_oh[row_idx]\n",
    "\n",
    "            rec_idx =  pred_row.argsort()[-recall_k:][::-1]\n",
    "            mrr_idx =  pred_row.argsort()[-mrr_k:][::-1]\n",
    "            tru_idx = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "            n += 1\n",
    "\n",
    "            if tru_idx[0] in rec_idx:\n",
    "                rec_sum += 1\n",
    "\n",
    "            if tru_idx[0] in mrr_idx:\n",
    "                mrr_sum += 1/int((np.where(mrr_idx == tru_idx[0])[0]+1))\n",
    "\n",
    "    recall = rec_sum/n\n",
    "    mrr = mrr_sum/n\n",
    "    return (recall, recall_k), (mrr, mrr_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0f97ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    train_dataset = SessionDataset(train_data)\n",
    "    model_to_train = model\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        with tqdm(total=train_samples_qty) as pbar:\n",
    "            loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "            for feat, target, mask in loader:\n",
    "\n",
    "                gru_layer = model_to_train.get_layer(name=\"GRU\")\n",
    "                hidden_states = gru_layer.states[0].numpy()\n",
    "                for elt in mask:\n",
    "                    hidden_states[elt, :] = 0\n",
    "                gru_layer.reset_states(states=hidden_states)\n",
    "\n",
    "                input_oh = to_categorical(feat, num_classes=loader.n_items)\n",
    "                input_oh = np.expand_dims(input_oh, axis=1)\n",
    "\n",
    "                target_oh = to_categorical(target, num_classes=loader.n_items)\n",
    "\n",
    "                tr_loss = model_to_train.train_on_batch(input_oh, target_oh)\n",
    "\n",
    "                pbar.set_description(\"Epoch {0}. Loss: {1:.5f}\".format(epoch, tr_loss))\n",
    "                pbar.update(loader.done_sessions_counter)\n",
    "\n",
    "        if save_weights:\n",
    "            print(\"Saving weights...\")\n",
    "            model_to_train.save('./GRU4REC_{}.h5'.format(epoch))\n",
    "\n",
    "        if eval_all_epochs:\n",
    "            (rec, rec_k), (mrr, mrr_k) = get_metrics(model_to_train, train_dataset.itemmap)\n",
    "            print(\"\\t - Recall@{} epoch {}: {:5f}\".format(rec_k, epoch, rec))\n",
    "            print(\"\\t - MRR@{}    epoch {}: {:5f}\\n\".format(mrr_k, epoch, mrr))\n",
    "\n",
    "    if not eval_all_epochs:\n",
    "        (rec, rec_k), (mrr, mrr_k) = get_metrics(model_to_train, train_dataset.itemmap)\n",
    "        print(\"\\t - Recall@{} epoch {}: {:5f}\".format(rec_k, epochs, rec))\n",
    "        print(\"\\t - MRR@{}    epoch {}: {:5f}\\n\".format(mrr_k, epochs, mrr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87b03701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(512, 1, 2934)]          0         \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    [(512, 100), (512, 100)]  910800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (512, 100)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (512, 2934)               296334    \n",
      "=================================================================\n",
      "Total params: 1,207,134\n",
      "Trainable params: 1,207,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4084cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 5.02643:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17148/17795 [00:22<00:00, 757.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 1: 0.498901\n",
      "\t - MRR@20    epoch 1: 0.251320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Loss: 4.80492:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17148/17795 [00:22<00:00, 769.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 2: 0.526245\n",
      "\t - MRR@20    epoch 2: 0.264528\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Loss: 4.61273:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17148/17795 [00:22<00:00, 749.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 3: 0.545166\n",
      "\t - MRR@20    epoch 3: 0.276505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4. Loss: 4.41084:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17148/17795 [00:21<00:00, 782.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 4: 0.561035\n",
      "\t - MRR@20    epoch 4: 0.287257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5. Loss: 4.28362:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17148/17795 [00:22<00:00, 768.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 5: 0.571777\n",
      "\t - MRR@20    epoch 5: 0.295560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6. Loss: 4.15981:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17148/17795 [00:23<00:00, 733.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 6: 0.584839\n",
      "\t - MRR@20    epoch 6: 0.302474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7. Loss: 4.02473:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17148/17795 [00:27<00:01, 614.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 7: 0.594116\n",
      "\t - MRR@20    epoch 7: 0.308263\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8. Loss: 3.89509:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17148/17795 [00:33<00:01, 508.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 8: 0.603394\n",
      "\t - MRR@20    epoch 8: 0.312562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9. Loss: 3.81041:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17148/17795 [00:36<00:01, 471.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 9: 0.609497\n",
      "\t - MRR@20    epoch 9: 0.317271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10. Loss: 3.74212:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:36<00:01, 469.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 10: 0.616943\n",
      "\t - MRR@20    epoch 10: 0.320180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11. Loss: 3.67324:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:36<00:01, 470.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 11: 0.621704\n",
      "\t - MRR@20    epoch 11: 0.324207\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12. Loss: 3.61325:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:45<00:01, 378.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 12: 0.625000\n",
      "\t - MRR@20    epoch 12: 0.326721\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13. Loss: 3.49706:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:32<00:01, 522.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 13: 0.628418\n",
      "\t - MRR@20    epoch 13: 0.327741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14. Loss: 3.42895:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:41<00:01, 412.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 14: 0.632446\n",
      "\t - MRR@20    epoch 14: 0.329261\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15. Loss: 3.43630:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:41<00:01, 412.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 15: 0.636353\n",
      "\t - MRR@20    epoch 15: 0.331410\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16. Loss: 3.37538:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:38<00:01, 447.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 16: 0.638550\n",
      "\t - MRR@20    epoch 16: 0.331869\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17. Loss: 3.26416:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:41<00:01, 412.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 17: 0.639893\n",
      "\t - MRR@20    epoch 17: 0.333670\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18. Loss: 3.25009:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:51<00:01, 331.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 18: 0.642334\n",
      "\t - MRR@20    epoch 18: 0.334164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19. Loss: 3.21333:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 17148/17795 [00:46<00:01, 367.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch 19: 0.642822\n",
      "\t - MRR@20    epoch 19: 0.335264\n",
      "\n"
     ]
    }
   ],
   "source": [
    " train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db63c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
